import streamlit as st
import tensorflow as tf
import numpy as np
from PIL import Image
import os
import cv2
import tempfile

# C·∫•u h√¨nh giao di·ªán
st.set_page_config(page_title="AI Recognition Pro", layout="centered")
st.title("üöÄ ·ª®ng d·ª•ng nh·∫≠n di·ªán ·∫¢nh & Video AI")
st.write("M√¥ h√¨nh s·ª≠ d·ª•ng: **MobileNetV2**")

# --- X·ª¨ L√ù M√î H√åNH ---
BASE_DIR = os.path.dirname(__file__)
# ƒê√£ s·ª≠a ƒë∆∞·ªùng d·∫´n theo t√™n th∆∞ m·ª•c 'model' c·ªßa b·∫°n
MODEL_PATH = os.path.join(BASE_DIR, "model", "MobileNetV2.keras")

@st.cache_resource
def load_model_ai():
    if not os.path.exists(MODEL_PATH):
        st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh t·∫°i: {MODEL_PATH}")
        return None
    try:
        # Load m√¥ h√¨nh v√† t·∫Øt compile ƒë·ªÉ tr√°nh l·ªói phi√™n b·∫£n th∆∞ vi·ªán
        return tf.keras.models.load_model(MODEL_PATH, compile=False)
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i file .keras: {e}")
        return None

model = load_model_ai()

# --- H√ÄM TI·ªÄN X·ª¨ L√ù ---
def prepare_image(img_pil):
    """Chuy·ªÉn ƒë·ªïi ·∫£nh v·ªÅ ƒë·ªãnh d·∫°ng MobileNetV2 y√™u c·∫ßu (224x224)"""
    img_resized = img_pil.resize((224, 224))
    img_array = tf.keras.preprocessing.image.img_to_array(img_resized)
    img_array = np.expand_dims(img_array, axis=0)
    return tf.keras.applications.mobilenet_v2.preprocess_input(img_array)

# --- GIAO DI·ªÜN T·∫¢I FILE ---
uploaded_file = st.file_uploader("K√©o th·∫£ ·∫¢nh ho·∫∑c Video v√†o ƒë√¢y", type=["jpg", "png", "jpeg", "mp4", "mov", "avi"])

if uploaded_file and model:
    # Ki·ªÉm tra xem l√† ·∫£nh hay video
    is_video = uploaded_file.type.startswith('video')

    if not is_video:
        # X·ª¨ L√ù ·∫¢NH
        image = Image.open(uploaded_file).convert('RGB')
        st.image(image, caption="·∫¢nh ƒë√£ t·∫£i l√™n", use_container_width=True)
        
        if st.button("üîç D·ª± ƒëo√°n ·∫¢nh"):
            with st.spinner("ƒêang ph√¢n t√≠ch..."):
                processed_img = prepare_image(image)
                preds = model.predict(processed_img)
                label = np.argmax(preds)
                score = np.max(preds) * 100
                
                st.divider()
                st.success(f"### K·∫øt qu·∫£: Nh√£n {label}")
                st.info(f"ƒê·ªô tin c·∫≠y: {score:.2f}%")
    
    else:
        # X·ª¨ L√ù VIDEO
        st.video(uploaded_file)
        if st.button("‚ñ∂Ô∏è Ph√¢n t√≠ch Video"):
            with st.spinner("ƒêang x·ª≠ l√Ω khung h√¨nh ch√≠nh..."):
                # T·∫°o file t·∫°m v√¨ OpenCV kh√¥ng ƒë·ªçc tr·ª±c ti·∫øp ƒë∆∞·ª£c file upload t·ª´ Streamlit
                t_file = tempfile.NamedTemporaryFile(delete=False)
                t_file.write(uploaded_file.read())
                
                cap = cv2.VideoCapture(t_file.name)
                # L·∫•y khung h√¨nh ·ªü gi·ªØa video ƒë·ªÉ c√≥ ƒë·ªô ch√≠nh x√°c cao nh·∫•t
                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)
                
                ret, frame = cap.read()
                if ret:
                    # Chuy·ªÉn BGR (OpenCV) sang RGB (PIL)
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    img_pil = Image.fromarray(frame_rgb)
                    
                    processed_frame = prepare_image(img_pil)
                    preds = model.predict(processed_frame)
                    label = np.argmax(preds)
                    score = np.max(preds) * 100
                    
                    st.divider()
                    st.success(f"### K·∫øt qu·∫£ Video: Nh√£n {label}")
                    st.info(f"ƒê·ªô tin c·∫≠y (t·∫°i khung h√¨nh gi·ªØa): {score:.2f}%")
                else:
                    st.error("Kh√¥ng th·ªÉ ƒë·ªçc ƒë∆∞·ª£c video n√†y.")
                
                cap.release()
                os.unlink(t_file.name) # X√≥a file t·∫°m ƒë·ªÉ nh·∫π b·ªô nh·ªõ
