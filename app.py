import streamlit as st
import tensorflow as tf
import numpy as np
from PIL import Image
import os
import cv2
import tempfile

# C·∫•u h√¨nh giao di·ªán chu·∫©n hi·ªán ƒë·∫°i
st.set_page_config(
    page_title="AI Vision Pro", 
    page_icon="ü§ñ", 
    layout="centered"
)

# Custom CSS ƒë·ªÉ giao di·ªán chuy√™n nghi·ªáp h∆°n
st.markdown("""
    <style>
    .main { background-color: #f5f7f9; }
    .stButton>button { width: 100%; border-radius: 20px; height: 3em; background-color: #007bff; color: white; }
    .stMetric { background-color: #ffffff; padding: 15px; border-radius: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }
    </style>
    """, unsafe_allow_html=True)

st.title("ü§ñ H·ªá th·ªëng Nh·∫≠n di·ªán ·∫¢nh & Video AI")
st.write("Gi·∫£i ph√°p ph√¢n t√≠ch h√¨nh ·∫£nh d·ª±a tr√™n ki·∫øn tr√∫c **MobileNetV2**.")

# --- X·ª¨ L√ù M√î H√åNH ---
BASE_DIR = os.path.dirname(__file__)
MODEL_PATH = os.path.join(BASE_DIR, "MobileNetV2.keras")

@st.cache_resource
def load_model_optimized():
    if not os.path.exists(MODEL_PATH):
        st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {MODEL_PATH}")
        return None
    try:
        # S·ª≠ d·ª•ng tf.keras ƒë·ªÉ load nh·∫±m kh·∫Øc ph·ª•c l·ªói xung ƒë·ªôt Layer tr√™n Keras 3
        model = tf.keras.models.load_model(MODEL_PATH, compile=False)
        return model
    except Exception as e:
        st.error(f"‚ùå L·ªói c·∫•u tr√∫c m√¥ h√¨nh: {e}")
        st.info("üí° M·∫πo: H·ªá th·ªëng ƒëang th·ª≠ x·ª≠ l√Ω xung ƒë·ªôt phi√™n b·∫£n Keras. H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ Reboot App sau khi s·ª≠a requirements.txt.")
        return None

model = load_model_optimized()

# --- KHU V·ª∞C CH·ª®C NƒÇNG ---
if model:
    tab1, tab2 = st.tabs(["üì∏ Ph√¢n t√≠ch ·∫¢nh", "üé• Ph√¢n t√≠ch Video"])

    with tab1:
        uploaded_img = st.file_uploader("K√©o th·∫£ ·∫£nh v√†o ƒë√¢y", type=["jpg", "png", "jpeg"], key="img")
        if uploaded_img:
            col1, col2 = st.columns([1, 1])
            with col1:
                img = Image.open(uploaded_img).convert('RGB')
                st.image(img, caption="·∫¢nh g·ªëc", use_container_width=True)
            
            with col2:
                if st.button("üöÄ B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n", key="btn_img"):
                    with st.spinner("ƒêang ph√¢n t√≠ch..."):
                        # Ti·ªÅn x·ª≠ l√Ω chu·∫©n MobileNetV2
                        img_input = np.array(img.resize((224, 224)))
                        img_input = tf.keras.applications.mobilenet_v2.preprocess_input(np.expand_dims(img_input, axis=0))
                        
                        preds = model.predict(img_input)
                        label = np.argmax(preds)
                        confidence = np.max(preds) * 100
                        
                        st.metric("Nh√£n d·ª± ƒëo√°n", f"S·ªë {label}")
                        st.metric("ƒê·ªô tin c·∫≠y", f"{confidence:.2f}%")
                        if confidence > 80: st.balloons()

    with tab2:
        uploaded_vid = st.file_uploader("T·∫£i video l√™n", type=["mp4", "mov", "avi"], key="vid")
        if uploaded_vid:
            st.video(uploaded_vid)
            if st.button("‚ñ∂Ô∏è Ph√¢n t√≠ch Video", key="btn_vid"):
                with st.spinner("ƒêang tr√≠ch xu·∫•t khung h√¨nh..."):
                    t_file = tempfile.NamedTemporaryFile(delete=False)
                    t_file.write(uploaded_vid.read())
                    
                    cap = cv2.VideoCapture(t_file.name)
                    # L·∫•y khung h√¨nh t·∫°i 50% th·ªùi l∆∞·ª£ng video
                    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)
                    
                    ret, frame = cap.read()
                    if ret:
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        img_input = np.array(Image.fromarray(frame_rgb).resize((224, 224)))
                        img_input = tf.keras.applications.mobilenet_v2.preprocess_input(np.expand_dims(img_input, axis=0))
                        
                        preds = model.predict(img_input)
                        st.success(f"### D·ª± ƒëo√°n: Nh√£n {np.argmax(preds)}")
                        st.progress(float(np.max(preds)))
                    else:
                        st.error("Kh√¥ng th·ªÉ x·ª≠ l√Ω video.")
                    cap.release()
                    os.unlink(t_file.name)

# --- CH√ÇN TRANG ---
st.divider()
st.caption("¬© 2026 AI Vision Pro - H·ªá th·ªëng v·∫≠n h√†nh tr√™n n·ªÅn t·∫£ng TensorFlow & Streamlit.")
