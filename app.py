import streamlit as st
import tensorflow as tf
import numpy as np
from PIL import Image
import os
import cv2 # Th∆∞ vi·ªán x·ª≠ l√Ω ·∫£nh v√† video

# C·∫•u h√¨nh trang
st.set_page_config(page_title="AI Image & Video Classifier", layout="centered")

st.title("üöÄ ·ª®ng d·ª•ng Nh·∫≠n di·ªán ·∫¢nh & Video AI")
st.write("Ph√¢n t√≠ch h√¨nh ·∫£nh v√† video ƒë·ªÉ d·ª± ƒëo√°n ƒë·ªëi t∆∞·ª£ng.")

# --- PH·∫¶N X·ª¨ L√ù ƒê∆Ø·ªúNG D·∫™N M√î H√åNH ---
BASE_DIR = os.path.dirname(__file__)
MODEL_PATH = os.path.join(BASE_DIR, "model", "MobileNetV2.keras")

@st.cache_resource
def load_model_safe():
    if not os.path.exists(MODEL_PATH):
        st.error(f"‚ö†Ô∏è L·ªói: Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh t·∫°i ƒë∆∞·ªùng d·∫´n: `{MODEL_PATH}`")
        st.info("M·∫πo: Ki·ªÉm tra th∆∞ m·ª•c 'model' v√† t√™n file 'MobileNetV2.keras' tr√™n GitHub.")
        return None
    try:
        model = tf.keras.model.load_model(MODEL_PATH, compile=False)
        return model
    except Exception as e:
        st.error(f"‚ùå Kh√¥ng th·ªÉ load m√¥ h√¨nh: {e}")
        return None

# Load m√¥ h√¨nh
model = load_model_safe()
if model is None:
    st.stop() # D·ª´ng ·ª©ng d·ª•ng n·∫øu m√¥ h√¨nh kh√¥ng load ƒë∆∞·ª£c

# --- TI·ªÄN X·ª¨ L√ù ·∫¢NH CHUNG ---
def preprocess_image_for_model(image_array, target_size=(224, 224)):
    """Ti·ªÅn x·ª≠ l√Ω m·∫£ng ·∫£nh cho MobileNetV2"""
    img_pil = Image.fromarray(image_array) # Chuy·ªÉn numpy array v·ªÅ PIL Image
    img_resized = img_pil.resize(target_size)
    img_array = tf.keras.preprocessing.image.img_to_array(img_resized)
    img_array = np.expand_dims(img_array, axis=0) # Th√™m batch dimension
    return tf.keras.applications.mobilenet_v2.preprocess_input(img_array)

# --- KHU V·ª∞C UPLOAD FILE ---
uploaded_file = st.file_uploader(
    "T·∫£i l√™n ·∫£nh (jpg, png) ho·∫∑c video (mp4, mov, avi)",
    type=["jpg", "png", "jpeg", "mp4", "mov", "avi"]
)

if uploaded_file is not None:
    file_type = uploaded_file.type.split('/')[0] # L·∫•y "image" ho·∫∑c "video"
    
    if file_type == "image":
        st.subheader("Ph√¢n t√≠ch ·∫£nh:")
        img = Image.open(uploaded_file).convert('RGB')
        st.image(img, caption="·∫¢nh b·∫°n ƒë√£ t·∫£i l√™n", use_container_width=True)
        
        # Ti·ªÅn x·ª≠ l√Ω v√† d·ª± ƒëo√°n
        img_array_np = np.array(img) # Chuy·ªÉn PIL Image sang numpy array
        processed_img = preprocess_image_for_model(img_array_np)
        
        if st.button("üîç D·ª± ƒëo√°n ·∫£nh"):
            with st.spinner('ƒêang ph√¢n t√≠ch ·∫£nh...'):
                prediction = model.predict(processed_img)
                class_idx = np.argmax(prediction)
                confidence = np.max(prediction) * 100
                
                st.divider()
                st.subheader("K·∫øt qu·∫£ d·ª± ƒëo√°n:")
                st.success(f"**Nh√£n d·ª± ƒëo√°n:** {class_idx}")
                st.info(f"**ƒê·ªô tin c·∫≠y:** {confidence:.2f}%")

    elif file_type == "video":
        st.subheader("Ph√¢n t√≠ch video:")
        st.video(uploaded_file) # Hi·ªÉn th·ªã video l√™n web
        
        # L∆∞u video t·∫°m th·ªùi ƒë·ªÉ OpenCV c√≥ th·ªÉ ƒë·ªçc
        t_file = tempfile.NamedTemporaryFile(delete=False) 
        t_file.write(uploaded_file.read())
        
        if st.button("‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu ph√¢n t√≠ch video (t·ª´ng khung h√¨nh)"):
            with st.spinner("ƒêang ph√¢n t√≠ch video... (Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t th·ªùi gian t√πy ƒë·ªô d√†i video)"):
                cap = cv2.VideoCapture(t_file.name)
                
                predictions_list = []
                frame_count = 0
                
                # T·∫°o placeholder ƒë·ªÉ c·∫≠p nh·∫≠t k·∫øt qu·∫£ li√™n t·ª•c
                prediction_text = st.empty()
                progress_bar = st.progress(0)

                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    # Chuy·ªÉn ƒë·ªïi m√†u t·ª´ BGR (OpenCV) sang RGB (TensorFlow/PIL)
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # Ti·ªÅn x·ª≠ l√Ω v√† d·ª± ƒëo√°n t·ª´ng khung h√¨nh
                    processed_frame = preprocess_image_for_model(frame_rgb)
                    prediction = model.predict(processed_frame, verbose=0) # verbose=0 ƒë·ªÉ tr√°nh in log nhi·ªÅu
                    
                    class_idx = np.argmax(prediction)
                    confidence = np.max(prediction)
                    
                    predictions_list.append((class_idx, confidence))
                    frame_count += 1
                    
                    # C·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh v√† k·∫øt qu·∫£ d·ª± ƒëo√°n
                    if frame_count % 10 == 0: # C·∫≠p nh·∫≠t m·ªói 10 frame ƒë·ªÉ kh√¥ng b·ªã qu√° t·∫£i
                        current_pred_idx, current_pred_conf = predictions_list[-1]
                        prediction_text.info(f"ƒêang x·ª≠ l√Ω khung h√¨nh {frame_count}... D·ª± ƒëo√°n hi·ªán t·∫°i: Nh√£n **{current_pred_idx}** (ƒê·ªô tin c·∫≠y: {current_pred_conf*100:.2f}%)")
                        progress_bar.progress(min(int(frame_count / cap.get(cv2.CAP_PROP_FRAME_COUNT) * 100), 100))
                
                cap.release()
                os.unlink(t_file.name) # X√≥a file t·∫°m th·ªùi
                
                st.divider()
                if predictions_list:
                    # Ph√¢n t√≠ch k·∫øt qu·∫£ t·ªïng th·ªÉ (v√≠ d·ª•: nh√£n xu·∫•t hi·ªán nhi·ªÅu nh·∫•t)
                    from collections import Counter
                    most_common_pred = Counter([p[0] for p in predictions_list]).most_common(1)[0]
                    st.success(f"Ph√¢n t√≠ch video ho√†n t·∫•t! Nh√£n xu·∫•t hi·ªán nhi·ªÅu nh·∫•t: **{most_common_pred[0]}** (s·ªë l·∫ßn: {most_common_pred[1]})")
                    st.info("ƒê·ªÉ ph√¢n t√≠ch chi ti·∫øt h∆°n (nh√£n thay ƒë·ªïi theo th·ªùi gian), b·∫°n c·∫ßn l∆∞u tr·ªØ v√† hi·ªÉn th·ªã k·∫øt qu·∫£ ph·ª©c t·∫°p h∆°n.")
                else:
                    st.warning("Kh√¥ng c√≥ khung h√¨nh n√†o ƒë∆∞·ª£c ph√¢n t√≠ch t·ª´ video.")

# --- CH√ö TH√çCH D∆Ø·ªöI TRANG ---
st.caption("L∆∞u √Ω: N·∫øu k·∫øt qu·∫£ ra nh√£n s·ªë, b·∫°n c·∫ßn t·∫°o danh s√°ch t√™n nh√£n ƒë·ªÉ hi·ªÉn th·ªã ch·ªØ.")
